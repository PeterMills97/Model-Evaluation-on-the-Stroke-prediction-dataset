# Model-Evaluation-on-the-Stroke-prediction-dataset
The motivation for this notebook came from my participation in Playground Series Season 3, Episode 2 Kaggle competition, which used a synthetic version of the Stroke Prediction dataset and raised some questions about model evaluation. The competition goal was the classification of stroke cases with the ROC_AUC metric used to evaluate participants entries on a hidden test set, generating public and private leader board positions. Considering the class imbalance in the data, this seemed a questionable choice as ROC_AUC is liable to give an overly optimistic picture in this situation. The aim for this notebook is to explore methods for model evaluation and potential issues that need to be anticipated. 
